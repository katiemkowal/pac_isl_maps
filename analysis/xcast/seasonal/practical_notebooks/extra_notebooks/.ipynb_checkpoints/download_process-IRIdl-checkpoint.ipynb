{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6ad039-4f46-4384-9cd9-47f2ff978d70",
   "metadata": {},
   "source": [
    "# Practicals: Backup Data Processing Plan, IRI Data Library\n",
    "Code to download seasonal data from IRI data library and process into netcdf if data is not available on ftp site or locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f959ab3-a663-459a-b03f-b0a2585135b1",
   "metadata": {},
   "source": [
    "## Conda Install\n",
    "This will require the following libraries to be installed in the conda environment, after checking they do not conflict with the int_desk train environment\n",
    "\n",
    "Run the following commands in terminal:\n",
    "\n",
    "conda activate intdesk_train\n",
    "\n",
    "conda install -c conda-forge -c iri-nextgen cptdl cptio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590892a1-23d3-4fc0-a967-c4498241ad7b",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49849242-250f-4236-b2be-1266c5529486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cptdl as dl \n",
    "import cptio as cio\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20a9574-7014-4260-9c80-0a9f99edf71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory Setup, pick your project folder\n",
    "project_dir = os.getcwd()#'/Users/katie/Desktop/trial_pacisl' #setup directory where you want to work for this project\n",
    "\n",
    "#make data directores if they don't exist\n",
    "os.makedirs(os.path.join(project_dir, 'practical_data'), exist_ok = True)\n",
    "data_dir = os.path.join(project_dir, 'practical_data')\n",
    "os.makedirs(os.path.join(data_dir, 'tsv_files'), exist_ok = True)\n",
    "os.makedirs(os.path.join(data_dir, 'nc_files'), exist_ok = True)\n",
    "tsv_dir = os.path.join(data_dir, 'tsv_files')\n",
    "nc_dir = os.path.join(data_dir, 'nc_files')\n",
    "\n",
    "#sets up a month naming directory to calculate target months based on initialization month and lead times\n",
    "number_to_month_name_dictionary = {\n",
    "    1: 'Jan',\n",
    "    2: 'Feb',\n",
    "    3: 'Mar',\n",
    "    4: 'Apr',\n",
    "    5: 'May',\n",
    "    6: 'Jun',\n",
    "    7: 'Jul',\n",
    "    8: 'Aug',\n",
    "    9: 'Sep',\n",
    "    10: 'Oct',\n",
    "    11: 'Nov',\n",
    "    12: 'Dec',\n",
    "    0: 'Dec'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f57a88-82e0-4046-9e2d-842eb89cf16d",
   "metadata": {},
   "source": [
    "## Download Setup from IRI Data Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c70b4f1-ca4a-43fa-bcc8-654d9e504f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dl.observations.keys() to see all options for predictand \n",
    "# and dl.hindcasts.keys() to see all options for predictors.\n",
    "# Make sure your first_year & final_year are compatible with \n",
    "# your selections for your predictors and predictands.\n",
    "gcms = ['GEOSS2S.PRCP','SPEAR.PRCP', 'CCSM4.PRCP', 'CanSIPSIC3.PRCP', 'CFSv2.PRCP']\n",
    "#predictand_name = 'UCSB0p05.PRCP' (UCSB0p05.PRCP is 5km data, UCSB.PRCP is 25' resolution data)\n",
    "predictand_name = 'UCSB0p05.PRCP'\n",
    "\n",
    "# 'fdate':\n",
    "#   The initialization date of the model forecasts / hindcasts.\n",
    "#   This field is defined by a python datetime.datetime object,\n",
    "#   for example: dt.datetime(2022, 5, 1) # YYYY, MM, DD as integers\n",
    "#   The year field is only used for forecasts, otherwise ignored.\n",
    "#   The day field is only used in subseasonal forecasts, otherwise ignored.\n",
    "#   The month field is an integer representing a month - ie, May=5.\n",
    "forecast_dates_of_interest = [(2023, 8, 1),(2023, 9, 1), (2023, 10, 1), (2023,11,1), (2024,1,1), (2024, 2, 1), (2023,3,1), (2024,5,1), (2024,6,1)]\n",
    "\n",
    "# lead_times will setup your target period of interest\n",
    "# lead_low will be the start and lead_high will be the end\n",
    "# for example, 1.5 and 3.5 for a forecast date of 2024,5,1 will be Jun-Aug\n",
    "lead_times = {\n",
    "'lead_low':[1.5,2.5,3.5],#1.5 month lead is the following month (e.g. for initialization in May, a forecast in June)\n",
    "'lead_high':[3.5,4.5,5.5]\n",
    "}\n",
    "\n",
    "download_args = { \n",
    "    # 'first_year':\n",
    "    #   The first year of hindcasts you want. **NOT ALL MODELS HAVE ALL YEARS**.\n",
    "    #   Double check that your model has hindcast data for all years in [first_year, final_year].\n",
    "    #   This field is defined by a python integer representing a year, e.g. 1993.\n",
    "    'first_year': 1993,  \n",
    "\n",
    "    # 'final_year':\n",
    "    #   The final year of hindcasts you want. **NOT ALL MODELS HAVE ALL YEARS**.\n",
    "    #   Double check that your model has hindcast data for all years in [first_year, final_year].\n",
    "    #   This field is defined by a python integer representing a year, e.g. 2016.\n",
    "    'final_year': 2017,  \n",
    "\n",
    "    # 'predictor_extent':\n",
    "    #   The geographic bounding box of the climate model data you want to download.\n",
    "    #   This field is defined by a python dictionary with the keys \"north\", \"south\",\n",
    "    #   \"east\", and \"west\", each of which maps to a python integer representing the \n",
    "    #   edge of a bounding box. i.e., \"north\" will be the northernmost boundary,\n",
    "    #   \"south\" the southernmost boundary.\n",
    "    #   Example: {\"north\": 90, \"south\": -90, \"east\": 0, \"west\": 180}\n",
    "    'predictor_extent': {\n",
    "        'west':  115,\n",
    "        'east': 260, \n",
    "        'north': 10,\n",
    "        'south': -30, \n",
    "      }, \n",
    "\n",
    "    # 'predictand_extent':\n",
    "    #   The geographic bounding box of the observation data you want to download.\n",
    "    #   This field is defined by a python dictionary with the keys \"north\", \"south\",\n",
    "    #   \"east\", and \"west\", each of which maps to a python integer representing the \n",
    "    #   edge of a bounding box. i.e., \"north\" will be the northernmost boundary,\n",
    "    #   \"south\" the southernmost boundary.\n",
    "    #   Example: {\"north\": 90, \"south\": -90, \"east\": 0, \"west\": 180}\n",
    "    'predictand_extent': {\n",
    "        'west':  145,\n",
    "        'east': 190,  \n",
    "        'north': 3,  \n",
    "        'south': -20, \n",
    "      },\n",
    "\n",
    "    # 'filetype':\n",
    "    #   The filetype to be downloaded. for now, it saves a lot of headache just to set this equal\n",
    "    #   to 'cptv10.tsv' which is a boutique plain-text CPT filetype based on .tsv + metadata.\n",
    "    'filetype': 'cptv10.tsv'    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb99394-0023-4ee1-ace2-b2a006029a11",
   "metadata": {},
   "source": [
    "## Download Observational Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33f154e-4d7f-4127-bf43-6c0f2ecc8396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug_ld1_UCSB0p05.PRCP\n",
      "/cpc/int_desk/pac_isl/analysis/xcast/seasonal/practical_notebooks/practical_data/nc_files/Aug_ld1_UCSB0p05.PRCP.nc\n",
      "STARTING DOWNLOAD OF Aug_ld1_UCSB0p05.PRCP\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTARTING DOWNLOAD OF \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obs_download_file))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tsv_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obs_download_file)))\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m---> 20\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(dl\u001b[38;5;241m.\u001b[39mobservations[predictand_name], os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tsv_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obs_download_file)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ml_download, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_dlauth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     Y \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m=\u001b[39m cio\u001b[38;5;241m.\u001b[39mopen_cptdataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tsv_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obs_download_file)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"
     ]
    }
   ],
   "source": [
    "for f in forecast_dates_of_interest:\n",
    "    initial_month = dt.datetime(*f).month\n",
    "    obs_leads = []\n",
    "    for l, lead_low in enumerate(lead_times['lead_low']):\n",
    "        start_time = time.time()\n",
    "        target_low = number_to_month_name_dictionary[(initial_month + lead_low - 0.5)%12]\n",
    "        target_high = number_to_month_name_dictionary[(initial_month + lead_times['lead_high'][l] - 0.5)%12]\n",
    "        l_download = download_args.copy()\n",
    "        l_download['target'] = '-'.join([target_low, target_high])\n",
    "        l_download['lead_low'] = lead_low\n",
    "        l_download['lead_high'] = lead_times['lead_high'][l]\n",
    "    \n",
    "        obs_download_file = '_'.join([number_to_month_name_dictionary[initial_month], 'ld' + str(l + 1), predictand_name])\n",
    "        print(obs_download_file)\n",
    "\n",
    "        print(os.path.join(nc_dir,'{}.nc'.format(obs_download_file)))\n",
    "        if not Path(os.path.join(nc_dir,'{}.nc'.format(obs_download_file))).is_file():\n",
    "            print('STARTING DOWNLOAD OF {}'.format(obs_download_file))\n",
    "            if not Path(os.path.join(tsv_dir, '{}.tsv'.format(obs_download_file))).is_file():\n",
    "                Y = dl.download(dl.observations[predictand_name], os.path.join(tsv_dir,'{}.tsv'.format(obs_download_file)), **l_download, verbose=True, use_dlauth=False)\n",
    "            else:\n",
    "                Y = g = cio.open_cptdataset(os.path.join(tsv_dir, '{}.tsv'.format(obs_download_file)))\n",
    "            Y = getattr(Y, [i for i in Y.data_vars][0]).expand_dims({'M':[predictand_name]})\n",
    "            Y = Y.sortby('X')\n",
    "            Y.to_netcdf(os.path.join(nc_dir,'{}.nc'.format(obs_download_file)))\n",
    "            print()\n",
    "        else:\n",
    "            Y = xr.open_dataset(os.path.join(nc_dir, '{}.nc'.format(obs_download_file)))\n",
    "            Y = getattr(Y, [i for i in Y.data_vars][0])\n",
    "        Y = Y.expand_dims({'L':[l+1]}).swap_dims({'T':'S'})\n",
    "        print('download time is ' + str(time.time() - start_time))\n",
    "        obs_leads.append(Y)\n",
    "        \n",
    "    #check all years are available for all lead times, only keep intersecting years\n",
    "    nyears = []\n",
    "    for obs in obs_leads:\n",
    "        nyears.append(len(np.unique(obs.S.values)))\n",
    "    if all(i==nyears[0] for i in nyears) == True:\n",
    "        obs_leads = xr.concat(obs_leads, dim = 'L')\n",
    "    else:\n",
    "        seas_years = []\n",
    "        for obs in obs_leads:\n",
    "            base_obs = obs.swap_dims({'S':'Ti'}).to_dataset(name = 'prec')\n",
    "            seas_years.append(np.unique(base_obs.Ti.dt.year.values))\n",
    "        intersecting_years = [x for x in seas_years[0] if x in seas_years[1] and x in seas_years[2]]\n",
    "\n",
    "        updated_obs = []\n",
    "        for obs in obs_leads:\n",
    "            obs_check = obs.swap_dims({'S':'Ti'}).to_dataset(name = 'prec')\n",
    "            updated_obs.append(obs_check.sel(Ti=obs_check.Ti.dt.year.isin(intersecting_years)).swap_dims({'Ti':'S'}).prec)\n",
    "        obs_leads = xr.concat(updated_obs, dim = 'L')\n",
    "\n",
    "    splot = obs_leads.isel(L=0).swap_dims({'S':'T'}).plot(col = 'T', col_wrap = 5, subplot_kws={'projection':ccrs.PlateCarree(central_longitude=180)})\n",
    "    for ax in  splot.axs.flat:\n",
    "        coasts = ax.coastlines()\n",
    "        ax.set_xlim(download_args['predictand_extent']['west'], download_args['predictand_extent']['east'])\n",
    "        ax.set_ylim(download_args['predictand_extent']['south'], download_args['predictand_extent']['north'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfecd6-910b-45aa-9f6a-de38186ddabc",
   "metadata": {},
   "source": [
    "## Download Model Data, Including Hindcasts and Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56a139d-6b0d-4b13-8bbb-57f7982e1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2023, 8, 1)\n",
      "Aug_ld1_GEOSS2S.PRCP_hind\n",
      "STARTING DOWNLOAD OF Aug_ld1_GEOSS2S.PRCP_hind\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tsv_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gcm_forecast_download_file)))\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTARTING DOWNLOAD OF \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gcm_hindcast_download_file))\n\u001b[0;32m---> 24\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(dl\u001b[38;5;241m.\u001b[39mhindcasts[gcm], os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tsv_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gcm_hindcast_download_file)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ml_download, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_dlauth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     f \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mdownload(dl\u001b[38;5;241m.\u001b[39mforecasts[gcm], os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tsv_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gcm_forecast_download_file)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39ml_download, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_dlauth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(g, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mdata_vars][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"
     ]
    }
   ],
   "source": [
    "# Download GCMs and save raw models to netcdf\n",
    "for f in forecast_dates_of_interest:\n",
    "    print(f)\n",
    "    iyear, imonth, iday = f # Create the datetime object for initialized data from f (stored as a tuple)\n",
    "    fdate = dt.datetime(iyear, imonth, iday)\n",
    "    initial_month = dt.datetime(*f).month\n",
    "    hindcast_data, forecast_data = [], []\n",
    "    for l, lead_low in enumerate(lead_times['lead_low']):\n",
    "        target_low = number_to_month_name_dictionary[(initial_month + lead_low - 0.5)%12]\n",
    "        target_high = number_to_month_name_dictionary[(initial_month + lead_times['lead_high'][l] - 0.5)%12]\n",
    "        l_download = download_args.copy()\n",
    "        l_download['target'] = '-'.join([target_low, target_high])\n",
    "        l_download['lead_low'] = lead_low\n",
    "        l_download['lead_high'] = lead_times['lead_high'][l]\n",
    "        l_download['fdate'] = fdate\n",
    "        hindcast_data_l, forecast_data_l = [], []\n",
    "        for gcm in gcms:\n",
    "            gcm_hindcast_download_file = '_'.join([number_to_month_name_dictionary[initial_month], 'ld' + str(l+1), gcm, 'hind'])\n",
    "            gcm_forecast_download_file = '_'.join([number_to_month_name_dictionary[initial_month], 'ld' + str(l+1), gcm, 'hind'])\n",
    "            if not Path(os.path.join(nc_dir,'{}.nc'.format(gcm_forecast_download_file))).is_file():\n",
    "                if not Path(os.path.join(tsv_dir,'{}.tsv'.format(gcm_forecast_download_file))).is_file():\n",
    "                    print('STARTING DOWNLOAD OF {}'.format(gcm_hindcast_download_file))\n",
    "                    g = dl.download(dl.hindcasts[gcm], os.path.join(tsv_dir,'{}.tsv'.format(gcm_hindcast_download_file)), **l_download, verbose=True, use_dlauth=False)\n",
    "                    f = dl.download(dl.forecasts[gcm], os.path.join(tsv_dir,'{}.tsv'.format(gcm_forecast_download_file)), **l_download, verbose=True, use_dlauth=False)\n",
    "                    g = getattr(g, [i for i in g.data_vars][0])\n",
    "                    f = getattr(f, [i for i in f.data_vars][0])\n",
    "                    print()\n",
    "                else:\n",
    "                    print('LOADING {} FROM FILE'.format(gcm_hindcast_download_file))\n",
    "                    g = cio.open_cptdataset(os.path.join(tsv_dir, '{}.tsv'.format(gcm_hindcast_download_file)))\n",
    "                    f = cio.open_cptdataset(os.path.join(tsv_dir, '{}.tsv'.format(gcm_forecast_download_file)))\n",
    "                    print()\n",
    "                    g = getattr(g, [i for i in g.data_vars][0])\n",
    "                    f = getattr(f, [i for i in f.data_vars][0])\n",
    "                #label datasets by model name\n",
    "                g = g.expand_dims({'M':[gcm]})\n",
    "                f = f.expand_dims({'M':[gcm]})\n",
    "\n",
    "                #regrid models to observation coordinates for consistent grids across models\n",
    "                #g = xc.regrid(g, Y.coords['X'].values, Y.coords['Y'].values)\n",
    "                #f = xc.regrid(f, Y.coords['X'].values, Y.coords['Y'].values)\n",
    "\n",
    "                nc_prep = xr.concat(g, 'M')\n",
    "                nc_prep.to_netcdf(os.path.join(nc_dir,'{}.nc'.format(gcm_hindcast_download_file)))\n",
    "                f_nc_prep = xr.concat(f, 'M')\n",
    "                f_nc_prep.to_netcdf(os.path.join(nc_dir, '{}.nc'.format(gcm_forecast_download_file)))\n",
    "            else:\n",
    "                g = xr.open_dataset(os.path.join(nc_dir,'{}.nc'.format(gcm_hindcast_download_file))).prec\n",
    "                f = xr.open_dataset(os.path.join(nc_dir,'{}.nc'.format(gcm_forecast_download_file))).prec\n",
    "            hindcast_data_l.append(g)\n",
    "            forecast_data_l.append(f)\n",
    "        hindcast_data_l = xr.concat(hindcast_data_l, dim = 'M')\n",
    "        forecast_data_l = xr.concat(forecast_data_l, dim = 'M')\n",
    "        hindcast_data_l = hindcast_data_l.expand_dims({'L':[l+1]}).swap_dims({'T':'S'})\n",
    "        forecast_data_l = forecast_data_l.expand_dims({'L':[l+1]}).swap_dims({'T':'S'})\n",
    "        hindcast_data.append(hindcast_data_l)\n",
    "        forecast_data.append(forecast_data_l)\n",
    "\n",
    "    forecast_data = xr.concat(forecast_data, dim = 'L')\n",
    "    #check all hindcast years are available for all lead times\n",
    "    nyears = []\n",
    "    for hcsts in hindcast_data:\n",
    "        nyears.append(len(np.unique(hcsts.S.values)))\n",
    "    if all(i==nyears[0] for i in nyears) == True:\n",
    "        hindcast_data = xr.concat(hindcast_data, dim = 'L')\n",
    "    else:\n",
    "        seas_years = []\n",
    "        for hcst in hindcast_data:\n",
    "            base_hcsts = hcst.swap_dims({'S':'Ti'}).to_dataset(name = 'prec')\n",
    "            seas_years.append(np.unique(base_hcsts.Ti.dt.year.values))\n",
    "        intersecting_years = [x for x in seas_years[0] if x in seas_years[1] and x in seas_years[2]]\n",
    "\n",
    "        updated_hcsts = []\n",
    "        for hcst in hindcast_data:\n",
    "            hcst_check = hcst.swap_dims({'S':'Ti'}).to_dataset(name = 'prec')\n",
    "            updated_hcsts.append(hcst_check.sel(Ti=hcst_check.Ti.dt.year.isin(intersecting_years)).swap_dims({'Ti':'S'}).prec)\n",
    "        hindcast_data = xr.concat(updated_hcsts, dim = 'L')\n",
    "    \n",
    "    \n",
    "    print('one month initialization')\n",
    "    splot = hindcast_data.isel(L = 0).swap_dims({'S':'T'}).isel(T=0).plot(col = 'M', col_wrap = 5, subplot_kws={'projection':ccrs.PlateCarree(central_longitude=180)})\n",
    "    for ax in  splot.axs.flat:\n",
    "        ax.set_xlim(download_args['predictand_extent']['west'], download_args['predictand_extent']['east'])\n",
    "        ax.set_ylim(download_args['predictand_extent']['south'], download_args['predictand_extent']['north'])\n",
    "        ax.coastlines(\"10m\", alpha=0.1, zorder=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88602a85-10f1-42b0-b9f5-d4f0111834a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcast_new",
   "language": "python",
   "name": "xcast_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
