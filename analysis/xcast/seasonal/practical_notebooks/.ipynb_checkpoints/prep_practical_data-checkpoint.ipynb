{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55ddebc7-9e8f-4ab9-aafe-59ba6e0f5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import xcast as xc\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import os\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import practical_helper_functions as helper\n",
    "\n",
    "# automatically reloads the configuration file once updated and saved so you don't have to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b302ba79-00e5-4365-aa59-7556eaa30584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the folder where you want to work for this project\n",
    "# either type in the location, or place this notebook in the folder where you want your project to live and set project_dir = os.getcwd()\n",
    "project_dir = os.getcwd()#\"/Users/katie/Desktop/trial_pacisl\" \n",
    "\n",
    "#make subdirectores to organize your work within the project if they don't already exist\n",
    "practical_data_dir = os.path.join(project_dir, 'practical_data')\n",
    "nmme_nc_dir = '/cpc/int_desk/pac_isl/data/processed/nmme/nc_files'\n",
    "cmorph_nc_dir = '/cpc/int_desk/pac_isl/data/processed/cmorph/nc_files'\n",
    "chirps_nc_dir = '/cpc/int_desk/pac_isl/data/processed/chirps/nc_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88b93065-73d8-49e9-a7a1-ef249a632e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dates = [(2023, 7,1), (2023, 8, 1), (2023, 9, 1), (2023, 10, 1), (2023, 11,1), (2023, 12,1),\n",
    "                 (2024, 1, 1), (2024, 2, 1), (2024, 3, 1), (2024, 4, 1), (2024, 5, 1), (2024, 6, 1)]\n",
    "\n",
    "gcms = ['NMME']\n",
    "\n",
    "#predictor extent, zone over which you want to train your model\n",
    "predictor_extent = {\n",
    "        'west':  135,\n",
    "        'east': 200,  \n",
    "        'north': 10,  \n",
    "        'south': -30\n",
    "      }\n",
    "\n",
    "#where you want to target your final analysis\n",
    "predictand_extent = {\n",
    "        'west':  153,\n",
    "        'east': 183,  \n",
    "        'north': 3,  \n",
    "        'south': -20\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1394a-7855-4d84-9385-0f8637ad87a0",
   "metadata": {},
   "source": [
    "### get all target / initialization periods for each initialization date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e64171ae-c5ce-4fab-80b7-5d65b6df2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_months, initial_month_names, target_seasons, target_months = [], [], [], []\n",
    " \n",
    "for i in initial_dates:\n",
    "    leads = [['1', '3'],['2', '4'], ['3','5']]\n",
    "    initial_month = dt.datetime(*i).month\n",
    "    initial_months.append(initial_month)\n",
    "    initial_month_names.append(helper.number_to_month_name_dictionary[initial_month])\n",
    "    target_month = []\n",
    "    target_seas = []\n",
    "    for l in leads:\n",
    "        target_low = helper.number_to_month_name_dictionary[(initial_month + float(l[0]))%12]\n",
    "        target_mid = helper.number_to_month_name_dictionary[(initial_month + float(l[0])+1)%12]\n",
    "        target_high = helper.number_to_month_name_dictionary[(initial_month + float(l[1]))%12]\n",
    "        target_seas.append('-'.join([target_low, target_high]))\n",
    "        target_month.append(target_low[0] + target_mid[0] + target_high[0])\n",
    "    target_seasons.append(target_seas)\n",
    "    target_months.append(target_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4ef99-25d0-493e-8ba0-a26f15194fb4",
   "metadata": {},
   "source": [
    "### setting up nmme data for analysis\n",
    "these netcdf files when processed are counting forwards, 32 years for single season analysis (1991-2022), 75 years for three season analysis (25 years times 3 seasons); set up this time as the base time to align the other files as the one 'T' index for each initialized month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9965696-5eab-4ee6-a5c8-486e6370aab9",
   "metadata": {},
   "source": [
    "#### one season setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fdf26a69-52f5-4dd7-b357-1bcf41ed9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in hindcast and forecast data\n",
    "training_length = 'one_seas'\n",
    "oneseas_hindcast_data_im, oneseas_forecast_data_im = [], []\n",
    "for i, im in enumerate(initial_month_names):\n",
    "    hindcast_data, forecast_data = [], []\n",
    "    for t, target in enumerate(target_months[i]):\n",
    "        hindcast_data_l, forecast_data_l = [], []\n",
    "        for gcm in gcms:\n",
    "            gcm_hindcast_download_file = '{}*.nc'.format('_'.join([im, 'ld' + leads[t][0], training_length, gcm, 'hind']))\n",
    "            gcm_forecast_download_file = '{}*.nc'.format('_'.join([im, 'ld' + leads[t][0], training_length, gcm, 'fcst']))\n",
    "            g = xr.open_dataset(glob.glob(os.path.join(nc_dir, gcm_hindcast_download_file))[0])\n",
    "            f = xr.open_dataset(glob.glob(os.path.join(nc_dir, gcm_forecast_download_file))[0])\n",
    "            g = helper.prep_names(g, helper.coordinate_conversion).expand_dims({'M':[gcm]}).dropna(dim = 'Y')\n",
    "            f = helper.prep_names(f, helper.coordinate_conversion).expand_dims({'M':[gcm]}).dropna(dim = 'Y')\n",
    "            hindcast_data_l.append(g)\n",
    "            forecast_data_l.append(f)\n",
    "        hindcast_data_l = xr.concat(hindcast_data_l, dim = 'M')\n",
    "        forecast_data_l = xr.concat(forecast_data_l, dim = 'M')\n",
    "        hindcast_data_l = hindcast_data_l.assign_coords({'L':t+1})\n",
    "        forecast_data_l = forecast_data_l.assign_coords({'L':t+1})\n",
    "        hindcast_data.append(hindcast_data_l)\n",
    "        forecast_data.append(forecast_data_l)\n",
    "    #create one dataset across all lead times of interest\n",
    "    forecast_data = xr.concat(forecast_data, dim = 'L')\n",
    "    #check all hindcast years are available for all lead times and only keep dataset with intersecting years\n",
    "    hindcast_data = xr.concat(hindcast_data, dim = 'L')\n",
    "\n",
    "    oneseas_hindcast_data_im.append(hindcast_data)\n",
    "    oneseas_forecast_data_im.append(forecast_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a9c21-93e3-4c27-9745-efd830c43d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneseas_hindcast_data_im[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df6b3a-02b8-4ea1-b848-f2ff999476ac",
   "metadata": {},
   "source": [
    "#### three season setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "830300b6-6eea-40a9-a2c1-fa94b82c5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in hindcast and forecast data\n",
    "training_length = 'three_seas'\n",
    "threeseas_hindcast_data_im, threeseas_forecast_data_im = [], []\n",
    "for i, im in enumerate(initial_month_names):\n",
    "    hindcast_data, forecast_data = [], []\n",
    "    for t, target in enumerate(target_months[i]):\n",
    "        hindcast_data_l, forecast_data_l = [], []\n",
    "        for gcm in gcms:\n",
    "            gcm_hindcast_download_file = '{}*.nc'.format('_'.join([im, 'ld' + leads[t][0], training_length, gcm, 'hind']))\n",
    "            gcm_forecast_download_file = '{}*.nc'.format('_'.join([im, 'ld' + leads[t][0], training_length, gcm, 'fcst']))\n",
    "            g = xr.open_dataset(glob.glob(os.path.join(nc_dir, gcm_hindcast_download_file))[0])\n",
    "            f = xr.open_dataset(glob.glob(os.path.join(nc_dir, gcm_forecast_download_file))[0])\n",
    "            g = helper.prep_names(g, helper.coordinate_conversion).expand_dims({'M':[gcm]}).dropna(dim = 'Y')\n",
    "            f = helper.prep_names(f, helper.coordinate_conversion).expand_dims({'M':[gcm]}).dropna(dim = 'Y')\n",
    "            hindcast_data_l.append(g)\n",
    "            forecast_data_l.append(f)\n",
    "        hindcast_data_l = xr.concat(hindcast_data_l, dim = 'M')\n",
    "        forecast_data_l = xr.concat(forecast_data_l, dim = 'M')\n",
    "        hindcast_data_l = hindcast_data_l.assign_coords({'L':t+1})\n",
    "        forecast_data_l = forecast_data_l.assign_coords({'L':t+1})\n",
    "        hindcast_data.append(hindcast_data_l)\n",
    "        forecast_data.append(forecast_data_l)\n",
    "    #create one dataset across all lead times of interest\n",
    "    forecast_data = xr.concat(forecast_data, dim = 'L')\n",
    "    #check all hindcast years are available for all lead times and only keep dataset with intersecting years\n",
    "    hindcast_data = xr.concat(hindcast_data, dim = 'L')\n",
    "    \n",
    "    #crop model data to predictor extent\n",
    "    hindcast_360 = helper.adjust_longitude_to_360(hindcast_data, 'X').sortby('Y', ascending = True).sortby('X', ascending = True)\n",
    "    hindcast_comp = hindcast_360.sel(X= slice(predictor_extent['west'], predictor_extent['east']),\n",
    "                            Y = slice(predictor_extent['south'], predictor_extent['north']))\n",
    "    forecast_360 = helper.adjust_longitude_to_360(forecast_data, 'X').sortby('Y', ascending = True).sortby('X', ascending = True)\n",
    "    forecast_comp = forecast_360.sel(X= slice(predictor_extent['west'], predictor_extent['east']),\n",
    "                        Y = slice(predictor_extent['south'], predictor_extent['north']))\n",
    "\n",
    "    threeseas_hindcast_data_im.append(hindcast_comp)\n",
    "    threeseas_forecast_data_im.append(forecast_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f13dfb-917e-4822-98dc-cb9ff04d74e1",
   "metadata": {},
   "source": [
    "### setting up cmorph data for analysis\n",
    "this data was processed in the same manner as nmme, so should have the exact same time stamp, but calculate the intersecting T values to be absolutely certain before combining across leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1a1ad9a-ffda-40f9-a590-bd96c5fa685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeseas_cmorph_im = []\n",
    "for i, im in enumerate(initial_month_names):\n",
    "    obs_leads = []\n",
    "    for t, target in enumerate(target_months[i]):\n",
    "        obs_file = '{}*.nc'.format('_'.join([im, 'ld' + str(t + 1), 'CMORPH']))\n",
    "        Y_raw = xr.open_dataset(glob.glob(os.path.join(nc_dir, obs_file))[0])\n",
    "        Y = helper.prep_names(Y_raw, helper.coordinate_conversion)\n",
    "        Y = getattr(Y, [i for i in Y.data_vars][0])\n",
    "        Y = Y.expand_dims({'L':[t+1], 'M':[0]}).to_dataset(name = 'precip')\n",
    "        obs_leads.append(Y)\n",
    "    obs_leads = xr.concat(obs_leads, dim = 'L')\n",
    "    \n",
    "    #crop observations to target zone\n",
    "    obs_360 = helper.adjust_longitude_to_360(obs_leads, 'X').sortby('Y', ascending = True).sortby('X', ascending = True)\n",
    "    obs_comp = obs_360.sel(X= slice(predictand_extent['west'], predictand_extent['east']),\n",
    "                            Y = slice(predictand_extent['south'], predictand_extent['north']))\n",
    "    threeseas_cmorph_im.append(obs_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef149f75-2a2b-4659-bb8a-dce55f5fd0f5",
   "metadata": {},
   "source": [
    "### setting up chirps data for analysis\n",
    "only keep years available to NMME: 1991-2016\n",
    "label all years as the NMME years to keep it simple (given that netcdf is counting forwards) once the years extracted are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44731113-994b-45f1-81a7-9ba49eda875e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'to_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mexpand_dims({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m:[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m0\u001b[39m]})\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto_dataset(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43moneseas_hindcast_data_im\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataset\u001b[49m(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#update observations to have same time dimension as the model hindcasts\u001b[39;00m\n\u001b[1;32m     15\u001b[0m Y_update \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/xcast_new/lib/python3.9/site-packages/xarray/core/common.py:272\u001b[0m, in \u001b[0;36mAttrAccessMixin.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m source[name]\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'to_dataset'"
     ]
    }
   ],
   "source": [
    "oneseas_ucsb_im = []\n",
    "for i, im in enumerate(initial_month_names):\n",
    "    obs_leads = []\n",
    "    for t, target in enumerate(target_months[i]):\n",
    "        obs_file = '{}*.nc'.format('_'.join([target, 'UCSB0p05_pac-islands']))\n",
    "        Y_raw = xr.open_dataset(glob.glob(os.path.join(nc_dir, obs_file))[0])\n",
    "        years_of_interest = helper.getYears(1991,2022)\n",
    "        Y_raw = Y_raw.sel(year=Y_raw.year.isin(years_of_interest))\n",
    "        Y = helper.prep_names(Y_raw, helper.coordinate_conversion)\n",
    "        Y = getattr(Y, [i for i in Y.data_vars][0])\n",
    "        Y = Y.expand_dims({'L':[t+1], 'M':[0]}).drop('season')\n",
    "        Y = Y.to_dataset(name = 'precip')\n",
    "        model = oneseas_hindcast_data_im[i].isel(L=t).to_dataset(name = 'precip')\n",
    "        #update observations to have same time dimension as the model hindcasts\n",
    "        Y_update = []\n",
    "        for oneyear, year in enumerate(Y.T.values):\n",
    "            Y_year = Y.sel(T=year)\n",
    "            Y_year = Y_year.assign_coords({'T': model.isel(T=oneyear).T.values})\n",
    "            Y_update.append(Y_year)\n",
    "        Y_update = xr.concat(Y_update, dim = 'T')\n",
    "        obs_leads.append(Y_update)\n",
    "    obs_leads = xr.concat(obs_leads, dim = 'L')\n",
    "    \n",
    "    #crop observations to target zone\n",
    "    obs_360 = helper.adjust_longitude_to_360(obs_leads, 'X').sortby('Y', ascending = True).sortby('X', ascending = True)\n",
    "    obs_comp = obs_360.sel(X= slice(predictand_extent['west'], predictand_extent['east']),\n",
    "                            Y = slice(predictand_extent['south'], predictand_extent['north']))\n",
    "    oneseas_ucsb_im.append(obs_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcast_new",
   "language": "python",
   "name": "xcast_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
