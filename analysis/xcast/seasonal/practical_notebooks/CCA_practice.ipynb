{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a861825-021d-42a6-babb-78879ee4189a",
   "metadata": {},
   "source": [
    "# Practical to Evaluate Using CCA As a Bias Correction Technique for Seasonal Forecasts\n",
    "In this practical, we will use some forecast models to generate some seasonal precipitation and sea surface temperature forecasts, and then assess how the raw precipitation forecasts compare two applications of CCA as a bias correction techniques **(1)** statistically correcting precipitation forecasts and **(2)** using teleconnection patterns with sea surface temperature (SST) to statistically correct raw SST forecasts and generate a bias-corrected precipitation forecast.\n",
    "\n",
    "The bias correction technique assessed is Canonical Correlation Analysis (CCA), which bias corrects forecasts by seeking correlation patterns across large spatial domains using Empirical Orthogonal Functions.\n",
    "\n",
    "The usefulness of this technique is especially sensitive to the extent of the spatial domain of the model used for training(predictor extent). We will test how varying the spatial extent of the predictors affects the performance of CCA as a bias correction technique.\n",
    "\n",
    "**This notebook should be run in the intdesk_train environment - check your kernel (upper righthand corner) is set to 'intdesk_train', so you have all necessary libraries**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c847f99-87fa-444c-a095-f07b094f2fc4",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb31a641-15a8-4ff8-a66e-c57724104987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xcast as xc\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import os\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from cartopy.feature import NaturalEarthFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26720fc-be27-4d2d-a635-c1fbbf44348b",
   "metadata": {},
   "source": [
    "## Project Directory Setup\n",
    "setup the folder where you want to work for this project\n",
    "\n",
    "either set project_dir equal to the location of your working directory, e.g. \"/Users/katie/Desktop/pacisl_training\" or place this notebook in the folder where you want your project to live and set project_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa135d11-5d1a-4d8f-b338-f3130941d0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Directory is Located in /Users/katie/Desktop/pacisl_training\n"
     ]
    }
   ],
   "source": [
    "project_dir = \"/Users/katie/Desktop/pacisl_training\"#os.getcwd()\n",
    "print('Project Directory is Located in ' + project_dir)\n",
    "\n",
    "#makes subdirectores to organize your work within the project if they don't already exist\n",
    "os.makedirs(os.path.join(project_dir, 'practical_data'), exist_ok = True)\n",
    "data_dir = os.path.join(project_dir, 'practical_data')\n",
    "os.makedirs(os.path.join(project_dir, 'practical_figures'), exist_ok = True)\n",
    "figure_dir = os.path.join(project_dir, 'practical_figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14e7c0-db4b-40ad-a8be-2254c08a55b3",
   "metadata": {},
   "source": [
    "### Add all of your data for this practical into your 'practical_data' folder now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12446c4e-96a0-48a5-82ad-3e8814c5c3f7",
   "metadata": {},
   "source": [
    "## Setup Your Constants: Spatial Extents and Dates\n",
    "\n",
    "**(1) initialization date**: Make sure this date is not set for a future month but in a present or past month, and then seasonal forecasts will be created for 3 target periods following that month. For instance if you pick (2023,8,1) as your initialization date, you will evaluate forecasts over Sep-Nov, Oct-Dec, and Dec-Feb.\n",
    "\n",
    "**(2) region of interest**: Several coordinates have been setup in this cell. Make sure your region_coords variable is equal to the name of one of the coordinate dictionary entries in this cell, and then name that region as you like, e.g. region_coords = solomon_coordinates, region_of_interest = 'Solomon Islands'.\n",
    "You can adjust the coordinate values as you like in the dictionaries if you want to play around with the predictand extent, just remember to keep the naming consistent.\n",
    "\n",
    "**(3) type of predictor**: This will either be set to 'sst' or 'precip', which will affect the type of forecast you generate, either using raw NMME SST predictions and statistically relating those to rainfall observations over a training period or raw NMME rainfall predictions and statistically relating those to rainfall observations over a training period.\n",
    "\n",
    "**(4) predictor extent**: This is the spatial domain of your model to train your data, and it will affect how the bias correction performs by creating a zone of interest upon which the model can train. A few zones of interest have been setup for you here, including 'tropicaloceans', 'pacific', and 'ENSO'. You will likely only need to play around with 'tropicaloceans' if you are generating an SST-based forecast. You can fine tune these parameters by adding additional coordinate dictionaries here or updating the ones created originally. Make sure to name the predictor extent at the end that aligns with your coordinate dictionary to you remember what you trained on when you save your files (the name is how the files will get saved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0ad1a4-596e-4576-988d-a0a71e5ca329",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PICK YOUR DATE you want to initialize the model, e.g. your current date\n",
    "initial_date = (2023, 8, 1)\n",
    "\n",
    "#REGION OF INTEREST\n",
    "#some predefined zones, update the numbers or add another zone following the same labelling structure\n",
    "pacislands_coordinates = {\n",
    "    'west': 130,\n",
    "    'east': 205,\n",
    "    'north': 8,\n",
    "    'south': -20\n",
    "    }\n",
    "\n",
    "chuuk_coordinates = {\n",
    "    'west': 151,\n",
    "    'east': 153,\n",
    "    'north': 8,\n",
    "    'south': 6\n",
    "    }\n",
    "\n",
    "fiji_coordinates = {\n",
    "    'west':  177,\n",
    "    'east': 182,  \n",
    "    'north': -15,  \n",
    "    'south': -20}\n",
    "\n",
    "kiribati_coordinates = {\n",
    "        'west':  202,\n",
    "        'east': 203,  \n",
    "        'north': 3,  \n",
    "        'south': 1}\n",
    "\n",
    "solomon_coordinates = {\n",
    "        'west':  155,\n",
    "        'east': 167,  \n",
    "        'north': -6,  \n",
    "        'south': -13}\n",
    "\n",
    "png_coordinates = {\n",
    "        'west':  130,\n",
    "        'east': 156,  \n",
    "        'north': 1,  \n",
    "        'south': -12}\n",
    "        \n",
    "palau_coordinates = {\n",
    "        'west':  133,\n",
    "        'east': 135,  \n",
    "        'north': 8,  \n",
    "        'south': 6\n",
    "}\n",
    "        \n",
    "vanuatu_coordinates = {\n",
    "        'west':  165,\n",
    "        'east': 170,  \n",
    "        'north': -12,  \n",
    "        'south': -20\n",
    "}\n",
    "\n",
    "samoa_coordinates = {\n",
    "        'west':  187,\n",
    "        'east': 191,  \n",
    "        'north': -13,  \n",
    "        'south': -15\n",
    "}\n",
    "\n",
    "tuvalu_coordinates = {\n",
    "        'west':  178,\n",
    "        'east': 180,  \n",
    "        'north': -8,  \n",
    "        'south': -9\n",
    "}\n",
    "\n",
    "\n",
    "### PICK YOUR TARGET REGION OF INTEREST\n",
    "region_of_interest = 'Vanuatu' #how you want to name your region (can include spaces)\n",
    "region_coords = vanuatu_coordinates #name of the coordinates to use for your region, as defined above\n",
    "\n",
    "### PICK YOUR PREDICTOR, either choose 'precip' or 'sst'\n",
    "predictor_type = 'precip'\n",
    "\n",
    "### PICK YOUR PREDICTOR TRAINING ZONE\n",
    "\n",
    "#some options outlined below\n",
    "#Pacific region, encompassing all islands\n",
    "pacific_extent = {\n",
    "    'west': 120,\n",
    "    'east': 210,\n",
    "    'north': 10,\n",
    "    'south': -30\n",
    "}\n",
    "\n",
    "#ENSO Nino34 region\n",
    "ENSO_extent = {\n",
    "    'west': 120,\n",
    "    'east': 180,\n",
    "    'north': 5,\n",
    "    'south': -5\n",
    "}\n",
    "\n",
    "#Global tropical oceans, including Pacific and Atlantic\n",
    "tropicaloceans_extent = {\n",
    "    'west': 120,\n",
    "    'east': 270,\n",
    "    'north': 20,\n",
    "    'south': -20\n",
    "}\n",
    "\n",
    "predictor_train_extent = pacific_extent\n",
    "predictor_train_extent_name = 'Pacific'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d1191-4bf0-4a35-a31e-a9792f9bd830",
   "metadata": {},
   "source": [
    "## Prepare Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985626f1-ffc1-42ee-8820-27bbde045840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target seasons to forecast\n",
      "['Sep-Nov', 'Oct-Dec', 'Nov-Jan']\n"
     ]
    }
   ],
   "source": [
    "#this cell is setup to calculate your target forecast months based on your initialization date\n",
    "#the forecast months are currently seto to be 1-3, 2-4 and 3-5 months ahead\n",
    "number_to_month_name_dictionary = {\n",
    "    1: 'Jan',\n",
    "    2: 'Feb',\n",
    "    3: 'Mar',\n",
    "    4: 'Apr',\n",
    "    5: 'May',\n",
    "    6: 'Jun',\n",
    "    7: 'Jul',\n",
    "    8: 'Aug',\n",
    "    9: 'Sep',\n",
    "    10: 'Oct',\n",
    "    11: 'Nov',\n",
    "    12: 'Dec',\n",
    "    0: 'Dec'\n",
    "}\n",
    "\n",
    "leads = [['1', '3'],['2', '4'], ['3','5']]\n",
    "initial_month = dt.datetime(*initial_date).month\n",
    "initial_month_name = number_to_month_name_dictionary[initial_month]\n",
    "target_seas = []\n",
    "for l in leads:\n",
    "    target_low = number_to_month_name_dictionary[(initial_month + float(l[0]))%12]\n",
    "    target_high = number_to_month_name_dictionary[(initial_month + float(l[1]))%12]\n",
    "    target_seas.append('-'.join([target_low, target_high]))\n",
    "print('Target seasons to forecast')\n",
    "print(target_seas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2eefb0-a2d8-48f5-b066-63b6effee8d3",
   "metadata": {},
   "source": [
    "### Load Observations and Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118dff77-e03d-4e32-9d96-a82cad4eaa87",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/katie/Desktop/pacisl_training/practical_data/Aug_threeseas_CMORPH_precip.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/katie/Desktop/pacisl_training/practical_data/Aug_threeseas_CMORPH_precip.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'a4bfda22-e207-44dc-9af0-e008e0fbe691']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs_leads \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minitial_month_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreeseas_CMORPH_precip.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#train the model on observations over a grid slightly larger than the region of interest\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#this could be updated for later fine-tuning, but calculated here to keep it simple for now\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictand_train_extent \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwest\u001b[39m\u001b[38;5;124m'\u001b[39m:  region_coords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meast\u001b[39m\u001b[38;5;124m'\u001b[39m: region_coords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meast\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m,  \n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorth\u001b[39m\u001b[38;5;124m'\u001b[39m: region_coords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m,  \n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msouth\u001b[39m\u001b[38;5;124m'\u001b[39m: region_coords[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msouth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/api.py:573\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    562\u001b[0m     decode_cf,\n\u001b[1;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    580\u001b[0m     backend_ds,\n\u001b[1;32m    581\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:646\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    645\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 646\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:409\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    403\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    404\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    405\u001b[0m )\n\u001b[1;32m    406\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    407\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    408\u001b[0m )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:356\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:418\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:412\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/intdesk_train/lib/python3.11/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2469\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2028\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/katie/Desktop/pacisl_training/practical_data/Aug_threeseas_CMORPH_precip.nc'"
     ]
    }
   ],
   "source": [
    "obs_leads = xr.open_dataset(os.path.join(data_dir, '_'.join([initial_month_name, 'threeseas_CMORPH_precip.nc'])))\n",
    "\n",
    "#train the model on observations over a grid slightly larger than the region of interest\n",
    "#this could be updated for later fine-tuning, but calculated here to keep it simple for now\n",
    "predictand_train_extent = {\n",
    "    'west':  region_coords['west']-5,\n",
    "        'east': region_coords['east']+5,  \n",
    "        'north': region_coords['north']+3,  \n",
    "        'south': region_coords['south']-2\n",
    "}\n",
    "obs_leads = obs_leads.sel(X=slice(predictand_train_extent['west'], predictand_train_extent['east']),\n",
    "                          Y=slice(predictand_train_extent['south'], predictand_train_extent['north']))\n",
    "print(obs_leads)\n",
    "\n",
    "# read in hindcast and forecast data\n",
    "hindcast_data = xr.open_dataset(os.path.join(data_dir, '_'.join([initial_month_name, 'threeseas_NMME_hcst', predictor_type + '.nc'])))\n",
    "forecast_data = xr.open_dataset(os.path.join(data_dir, '_'.join([initial_month_name, 'threeseas_NMME_fcst', predictor_type + '.nc'])))\n",
    "\n",
    "hindcast_data = hindcast_data.sel(X=slice(predictor_train_extent['west'], predictor_train_extent['east']),\n",
    "                          Y=slice(predictor_train_extent['south'], predictor_train_extent['north']))\n",
    "forecast_data = forecast_data.sel(X=slice(predictor_train_extent['west'], predictor_train_extent['east']),\n",
    "                          Y=slice(predictor_train_extent['south'], predictor_train_extent['north']))\n",
    "\n",
    "#read in the ocean mask for the region (makes it easier to see the plots)\n",
    "msk = xr.open_dataset(os.path.join(data_dir, 'pacific_mask.nc'))\n",
    "mskk = msk.amask.expand_dims({'M':[0]})\n",
    "mskk = mskk.assign_coords({'lon': [i + 360 if i <= 0 else i for i in mskk.coords['lon'].values]}).sortby('lon').drop_duplicates('lon')\n",
    "mskk = mskk.rename({'lon':'X', 'lat':'Y', 'time':'T'})\n",
    "mskk = xc.regrid(mskk, obs_leads.X, obs_leads.Y)\n",
    "mask_missing = mskk.mean('T', skipna=False).mean('M', skipna=False)\n",
    "mask_missing = xr.ones_like(mask_missing).where(~np.isnan(mask_missing), other=np.nan )\n",
    "\n",
    "if region_coords == chuuk_coordinates:\n",
    "    obs_leads = obs_leads.copy()\n",
    "else:\n",
    "    obs_leads = obs_leads * mask_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9bfe9-f71b-4ed0-a652-edd112a5c2ea",
   "metadata": {},
   "source": [
    "#### Check your region of interest is what you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578c4e5-0668-44a7-b76e-19ef692acfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_check = obs_leads.sel(X=slice(predictand_train_extent['west'], predictand_train_extent['east']),\n",
    "                          Y=slice(predictand_train_extent['south'], predictand_train_extent['north']))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6,2), \n",
    "                         subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180)})\n",
    "\n",
    "# Your plotting code here using the specific model and season\n",
    "xplot = obs_check.isel(T=0, L=2).precip.plot(ax=axes,\n",
    "                                 transform=ccrs.PlateCarree())\n",
    "axes.coastlines()\n",
    "c = axes.coastlines()\n",
    "# Add country borders\n",
    "axes.add_feature(NaturalEarthFeature(category='cultural', name='admin_0_countries', \n",
    "                                    scale='50m', edgecolor='black', facecolor='none'))\n",
    "# Set the extent to cover the specific area\n",
    "axes.set_extent([region_coords['west'], region_coords['east'], region_coords['south'], region_coords['north']], crs=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce233179-e769-4dc8-8f4e-e41fe94da5e2",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ea57-1c92-4ed8-b932-bad0074790eb",
   "metadata": {},
   "source": [
    "### Bias correct raw model outputs using Canonical Correlation Analysis (CCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9718bfe-bf99-440c-a660-d8d9dc980c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "cca_fcsts_prob, cca_fcsts_det, cca_hcasts_det, cca_hcasts_prob, obs_to_test, raw_to_test = [],[],[],[],[],[]\n",
    "y_cca_loadings_test, x_cca_loadings_test, y_eof_loadings_test, x_eof_loadings_test = [],[],[],[]\n",
    "\n",
    "for l in np.unique(hindcast_data.L):\n",
    "    obs = obs_leads.sel(L=l).precip\n",
    "    model = hindcast_data.sel(L=l)[predictor_type]\n",
    "    fmodel = forecast_data.sel(L=l)[predictor_type]\n",
    "\n",
    "    #run CCA\n",
    "    hindcasts_det, hindcasts_prob, obs_test, raw_test, y_cca_loadings, x_cca_loadings, y_eof_loadings, x_eof_loadings = [], [], [], [], [], [], [], []\n",
    "    i=1\n",
    "    for xtrain, ytrain, xtest, ytest in xc.CrossValidator(model, obs, window=5):\n",
    "        print(\"window {}\".format(i))\n",
    "        i += 1\n",
    "        reg = xc.CCA(search_override=(5,\n",
    "                                      5,\n",
    "                                     3))\n",
    "        reg.fit(xtrain, ytrain)\n",
    "        preds = reg.predict(xtest)\n",
    "        probs =  reg.predict_proba(xtest)\n",
    "        obs_test.append(ytest)\n",
    "        raw_test.append(xtest)\n",
    "        hindcasts_det.append(preds)\n",
    "        hindcasts_prob.append(probs)\n",
    "        x_cca_loadings.append(reg.x_cca_loadings)\n",
    "        y_cca_loadings.append(reg.y_cca_loadings)\n",
    "        x_eof_loadings.append(reg.x_eof_loadings)\n",
    "        y_eof_loadings.append(reg.y_eof_loadings)\n",
    "    hindcasts_det = xr.concat(hindcasts_det, 'T')\n",
    "    hindcasts_prob = xr.concat(hindcasts_prob, 'T')\n",
    "    obs_test = xr.concat(obs_test, 'T')\n",
    "    raw_test = xr.concat(raw_test, 'T')\n",
    "    x_cca_loadings = xr.concat(x_cca_loadings, 'T')\n",
    "    y_cca_loadings = xr.concat(y_cca_loadings, 'T')\n",
    "    x_eof_loadings = xr.concat(x_eof_loadings, 'T')\n",
    "    y_eof_loadings = xr.concat(y_eof_loadings, 'T')\n",
    "    \n",
    "    fprobs =  reg.predict_proba(fmodel)\n",
    "    \n",
    "    cca_fcsts_prob.append(fprobs)\n",
    "    cca_hcasts_det.append(hindcasts_det)\n",
    "    cca_hcasts_prob.append(hindcasts_prob)\n",
    "    obs_to_test.append(obs_test)\n",
    "    raw_to_test.append(raw_test)\n",
    "    y_cca_loadings_test.append(y_cca_loadings)\n",
    "    x_cca_loadings_test.append(x_cca_loadings)\n",
    "    y_eof_loadings_test.append(y_eof_loadings)\n",
    "    x_eof_loadings_test.append(x_eof_loadings)\n",
    "cca_fcsts_prob = xr.concat(cca_fcsts_prob, dim = 'L')\n",
    "cca_hcasts_det = xr.concat(cca_hcasts_det, dim = 'L')\n",
    "cca_hcasts_prob = xr.concat(cca_hcasts_prob, dim = 'L')\n",
    "obs_to_test = xr.concat(obs_to_test, dim = 'L')\n",
    "raw_to_test = xr.concat(raw_to_test, dim = 'L')\n",
    "y_cca_loadings_test = xr.concat(y_cca_loadings_test, dim = 'L')\n",
    "x_cca_loadings_test = xr.concat(x_cca_loadings_test, dim = 'L')\n",
    "y_eof_loadings_test = xr.concat(y_eof_loadings_test, dim = 'L')\n",
    "x_eof_loadings_test = xr.concat(x_eof_loadings_test, dim = 'L')\n",
    "print('cca processing time is ' + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563b1ae-9ae0-4279-af94-347dd875fae5",
   "metadata": {},
   "source": [
    "### Examine EOF and CCA Loadings for both Model and Observations - \n",
    "#### Where in space are the model predictions (i.e. predictors) and observations (i.e. predictands) more highly correlated over the training period?\n",
    "CCA works by finding locations over a large spatial domain that have the highest correlation with the target predictand zone.\n",
    "\n",
    "Empirical Orthogonal Functions (EOFs) condense the coordinate data from X,Y space into a one-dimensional time series to test the correlations between the predictor space (model data) and predictand space (observed data)\n",
    "\n",
    "Examining the loadings for both the EOFs and the correlations between EOFs will help visualize where in space the CCA algorithm is picking up locations of interest to bias correct the raw model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dade67d-5933-4f9b-b1a4-93132352b5c3",
   "metadata": {},
   "source": [
    "#### Pick Which Loadings to Examine\n",
    "Select which loadings you want to examine, loadings can be either\n",
    "\n",
    "**x_eof_loadings_test**: EOF Loadings for Model Over the Predictor Space\n",
    "\n",
    "**y_eof_loadings_test**: EOF Loadings for Observations Over the Predictand Space\n",
    "\n",
    "**x_cca_loadings_test**: CCA Coefficients (Loadings) for the Modelled Time Series Projected back onto the Model EOF Loadings over Predictor Space\n",
    "\n",
    "**y_cca_loadings_test**: CCA Coefficients (Loadings) for the Observed Time Series Projected back onto the Observed EOF Loadings over Predictand Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ee6db1-a6b1-42cd-9953-c5d8c9f8e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICK ONE: eof or cca\n",
    "loadings_type = 'cca'\n",
    "#PICK ONE: model or observations\n",
    "target_focus = 'observations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b5188-a7e6-43c8-bfb2-e03187f94951",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dict = {\n",
    "    'model': 'x',\n",
    "    'observations': 'y'\n",
    "}\n",
    "loadings_test = globals()[('_'.join([predict_dict[target_focus], loadings_type, 'loadings_test']))]\n",
    "print('Examining ' + ('_'.join([predict_dict[target_focus], loadings_type, 'loadings_test'])))\n",
    "\n",
    "if target_focus == 'observations':\n",
    "    spatial_extent = region_coords.copy()\n",
    "elif target_focus == 'model':\n",
    "    spatial_extent = predictor_train_extent.copy()\n",
    "    \n",
    "modes = np.unique(loadings_test.mode.values)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(modes), ncols=len(target_seas), figsize=(10, (len(modes)-1)*2 + 2), \n",
    "                         subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180)})\n",
    "\n",
    "# Set the extent to cover the entire world\n",
    "for ax in axes.flat:\n",
    "    ax.set_global()\n",
    "\n",
    "for j, mode in enumerate(modes):\n",
    "    for i, season in enumerate(target_seas):\n",
    "        ax = axes[j, i]\n",
    "        # Your plotting code here using the specific mode and season\n",
    "        xplot = loadings_test.mean(dim = 'T').isel(L=i, mode=j).plot(ax=ax,\n",
    "                                              transform=ccrs.PlateCarree(),\n",
    "                                              cmap='coolwarm', levels=21, add_colorbar=False)\n",
    "        ax.coastlines()\n",
    "        c = ax.coastlines()\n",
    "        c = ax.gridlines(draw_labels=True, linewidth=0.3)\n",
    "        c.right_labels = False\n",
    "        c.top_labels = False \n",
    "        c.bottom_labels = False\n",
    "        \n",
    "        # Add country borders\n",
    "        ax.add_feature(NaturalEarthFeature(category='cultural', name='admin_0_countries', \n",
    "                                            scale='50m', edgecolor='black', facecolor='none'))\n",
    "        # Set the extent to cover the specific area\n",
    "        ax.set_extent([spatial_extent['west'], spatial_extent['east'], spatial_extent['south'], spatial_extent['north']], crs=ccrs.PlateCarree())\n",
    "        ax.set_title(None)\n",
    "        ax.set_title('Mode ' + f'{mode} - {season}')\n",
    "        \n",
    "# Add a single horizontal colorbar below the panel plot\n",
    "cbar_ax = fig.add_axes([0.15, 0.002, 0.6, 0.02])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(xplot, cax=cbar_ax, orientation='horizontal', shrink =1, pad = 0.3)\n",
    "cbar.set_label(region_of_interest + ' ' + target_focus.capitalize() + ' ' + loadings_type.upper() + ' Loadings', fontsize=13)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(left=0.05, right=0.9, top=0.95, bottom=0.07, wspace=0.01, hspace=0.2)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "#To save plot replace plt.show() with plt.savefig(os.path.join(figure_dir, '_'.join([initial_month_name, region_of_interest, target_focus.upper(), loadings_type.upper(), predictor_type, predictor_train_extent_name, LOADINGS_CCA'])), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90467a-102b-4de9-aac8-d9534a6318ff",
   "metadata": {},
   "source": [
    "## Evaluate Performance of Raw vs Bias Corrected Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4999c5-7eb3-49c6-ac71-79e8d2ebc89c",
   "metadata": {},
   "source": [
    "#### Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5c6aa7-21e1-400e-a8d2-2142c947dd1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hindcast_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#calculate pearson correlation score for hindcasts\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pearson_cca, pearson_raw \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, lead \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(\u001b[43mhindcast_data\u001b[49m\u001b[38;5;241m.\u001b[39mL\u001b[38;5;241m.\u001b[39mvalues)):\n\u001b[1;32m      5\u001b[0m     cca_pearson_calc \u001b[38;5;241m=\u001b[39m xc\u001b[38;5;241m.\u001b[39mPearson(cca_hcasts_det\u001b[38;5;241m.\u001b[39misel(L\u001b[38;5;241m=\u001b[39ml),obs_to_test\u001b[38;5;241m.\u001b[39misel(L\u001b[38;5;241m=\u001b[39ml))\n\u001b[1;32m      6\u001b[0m     cca_pearson_calc \u001b[38;5;241m=\u001b[39m cca_pearson_calc\u001b[38;5;241m.\u001b[39mexpand_dims({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMME CCA\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hindcast_data' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#calculate pearson correlation score for hindcasts\n",
    "pearson_cca, pearson_raw = [], []\n",
    "for l, lead in enumerate(np.unique(hindcast_data.L.values)):\n",
    "    cca_pearson_calc = xc.Pearson(cca_hcasts_det.isel(L=l),obs_to_test.isel(L=l))\n",
    "    cca_pearson_calc = cca_pearson_calc.expand_dims({'M':['NMME CCA']})\n",
    "\n",
    "    #regrid raw data for pearson calculation on one to one grid\n",
    "    raw_regrid = xc.regrid(hindcast_data.isel(L=l).precip, obs_leads.X, obs_leads.Y)\n",
    "    hindcast_raw = raw_regrid * mask_missing\n",
    "    \n",
    "    #calc pearson correlation\n",
    "    pearson_raw_calc = []\n",
    "    for m, model in enumerate(np.unique(hindcast_raw.M.values)):\n",
    "        pearson_raw_c = xc.Pearson(hindcast_raw.sel(M=model).expand_dims({'M':[model]}), \n",
    "                                           obs_leads.isel(L=l).precip)\n",
    "        pearson_raw_c = pearson_raw_c.expand_dims({'M':[model + ' Raw']})\n",
    "        pearson_raw_calc.append(pearson_raw_c)\n",
    "    pearson_raw_calc = xr.concat(pearson_raw_calc, dim = 'M')\n",
    "    pearson_cca.append(cca_pearson_calc)\n",
    "    pearson_raw.append(pearson_raw_calc)\n",
    "pearson_cca = xr.concat(pearson_cca, dim = 'L')\n",
    "pearson_raw = xr.concat(pearson_raw, dim = 'L')\n",
    "pearsons = xr.concat([pearson_cca, pearson_raw], dim = 'M')\n",
    "print('Pearson processing time is ' + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b3688-b777-4b95-9738-8fcd9d38bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.unique(pearsons.M.values)\n",
    "models = np.flip(models, axis = 0)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(models), ncols=len(target_seas), figsize=(10, (2)*2 + 2), \n",
    "                         subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180)})\n",
    "\n",
    "# Set the extent to cover the entire world\n",
    "for ax in axes.flat:\n",
    "    ax.set_global()\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    for i, season in enumerate(target_seas):\n",
    "        ax = axes[j, i]\n",
    "        # Your plotting code here using the specific model and season\n",
    "        xplot = pearsons.isel(L=i, M=j).plot(ax=ax,\n",
    "                                              transform=ccrs.PlateCarree(),\n",
    "                                              cmap='coolwarm', levels=21, vmin=-1, vmax=1, add_colorbar=False)\n",
    "        ax.coastlines()\n",
    "        c = ax.coastlines()\n",
    "        c = ax.gridlines(draw_labels=True, linewidth=0.3)\n",
    "        c.right_labels = False\n",
    "        c.top_labels = False \n",
    "        # Add country borders\n",
    "        ax.add_feature(NaturalEarthFeature(category='cultural', name='admin_0_countries', \n",
    "                                            scale='50m', edgecolor='black', facecolor='none'))\n",
    "        # Set the extent to cover the specific area\n",
    "        ax.set_extent([region_coords['west'], region_coords['east'], region_coords['south'], region_coords['north']], crs=ccrs.PlateCarree())\n",
    "        ax.set_title(f'{model} - {season}')\n",
    "\n",
    "# Add a single horizontal colorbar below the panel plot\n",
    "cbar_ax = fig.add_axes([0.15, 0.002, 0.6, 0.02])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(xplot, cax=cbar_ax, orientation='horizontal', shrink =1, pad = 0.3)\n",
    "cbar.set_label(region_of_interest + ' Pearson Correlation', fontsize=13)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(left=0.05, right=0.9, top=0.95, bottom=0.07, wspace=0.01, hspace=0.2)\n",
    "\n",
    "# Show plot\n",
    "plt.savefig(os.path.join(figure_dir, '_'.join([initial_month_name, region_of_interest, predictor_type, predictor_train_extent_name, 'pearson_CCA'])), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd75b53-2e74-48eb-b1cf-bf22eb645122",
   "metadata": {},
   "source": [
    "#### GROCS\n",
    "to run this code we need a couple of functions to help transform the observational data into tercile categories\n",
    "make sure to add 'onehotupdate.py' to the function folder where you pull it from in the next step. You can make the function folder the same folder as your project directory, just make sure the folder name is where this file is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e20be-54ed-486a-b07b-2d937ccb35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the necessary functions\n",
    "import importlib.util\n",
    "\n",
    "#onehotupdate.py file and where it is located\n",
    "function_folder = \"/cpc/int_desk/pac_isl/analysis/xcast/seasonal/onehotupdate.py\"\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "\"onehotupdate\", function_folder)    \n",
    "\n",
    "onehot = importlib.util.module_from_spec(spec) \n",
    "spec.loader.exec_module(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b3a15-a32e-464f-b954-1819c04ed0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grocs_cca = []\n",
    "for l, lead in enumerate(np.unique(hindcast_data.L.values)):\n",
    "\n",
    "    hind_prob = xc.gaussian_smooth(cca_hcasts_prob.isel(L=l), kernel=3)\n",
    "    obs = xc.gaussian_smooth(obs_to_test.isel(L=l), kernel=3)\n",
    "\n",
    "    #transform obs into tercile based categories\n",
    "    ohc = onehot.OneHotEncoder() \n",
    "    ohc.fit(obs)\n",
    "    T = ohc.transform(obs)\n",
    "    clim = xr.ones_like(T) * 0.333\n",
    "    \n",
    "    grocs_cca_l = xc.GROCS(hind_prob, T)\n",
    "    grocs_cca_l = grocs_cca_l.expand_dims({'M':['NMME CCA']})\n",
    "    grocs_cca.append(grocs_cca_l)\n",
    "\n",
    "grocs_cca = xr.concat(grocs_cca, dim = 'L')\n",
    "grocs = grocs_cca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d2002-333f-496d-9b4c-502957d607c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.unique(grocs.M.values)\n",
    "print(target_seas)\n",
    "fig, axes = plt.subplots(nrows = len(models), ncols=len(target_seas), figsize=(10, (1)*2 + 2), \n",
    "                         subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180)})\n",
    "\n",
    "# Set the extent to cover the entire world\n",
    "for ax in axes.flat:\n",
    "    ax.set_global()\n",
    "for j, model in enumerate(grocs.M.values):\n",
    "    for i, season in enumerate(target_seas):\n",
    "        ax = axes[i]\n",
    "        # Your plotting code here using the specific model and season\n",
    "        xplot = grocs.isel(L=i, M=j).plot(ax=ax, transform=ccrs.PlateCarree(),\n",
    "                                              cmap='coolwarm', levels=21, vmin=0, vmax=1, add_colorbar=False)\n",
    "        ax.coastlines()\n",
    "        c = ax.coastlines()\n",
    "        c = ax.gridlines(draw_labels=True, linewidth=0.3)\n",
    "        c.right_labels = False\n",
    "        c.top_labels = False \n",
    "        # Add country borders\n",
    "        ax.add_feature(NaturalEarthFeature(category='cultural', name='admin_0_countries', \n",
    "                                            scale='50m', edgecolor='black', facecolor='none'))\n",
    "        # Set the extent to cover the specific area\n",
    "        ax.set_extent([region_coords['west'], region_coords['east'], region_coords['south'], region_coords['north']], crs=ccrs.PlateCarree())\n",
    "        ax.set_title(f'{model} - {season}')\n",
    "\n",
    "# Add a single horizontal colorbar below the panel plot\n",
    "cbar_ax = fig.add_axes([0.15, 0.002, 0.6, 0.02])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(xplot, cax=cbar_ax, orientation='horizontal', shrink =1, pad = 0.3)\n",
    "cbar.set_label(region_of_interest + ' GROCS', fontsize=13)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(left=0.05, right=0.9, top=0.95, bottom=0.05, wspace=0.01, hspace=0.2)\n",
    "\n",
    "# Show plot\n",
    "plt.savefig(os.path.join(figure_dir, '_'.join([initial_month_name, region_of_interest, predictor_type, predictor_train_extent_name, 'GROCS_CCA'])), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e9ed1-31b5-4a9d-b1d0-7290cc2ce307",
   "metadata": {},
   "source": [
    "## Plot some Probabalistic Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f2f19-4780-4dad-9da3-5ec35e01a275",
   "metadata": {},
   "source": [
    "### Plot the Bias Corrected Forecasts using CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4f3c7-9ea6-43de-9efd-48e42b82752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, lead in enumerate(np.unique(cca_fcsts_prob.L)):\n",
    "    im = xc.view_probabilistic(cca_fcsts_prob.isel(T=0, L=l).sel(X=slice(region_coords['west'], region_coords['east']),\n",
    "                                                                   Y=slice(region_coords['south'], region_coords['north'])), cross_dateline=True,\n",
    "                             title= region_of_interest + ' CCA MME Probabalistic Forecast for ' + target_seas[l],\n",
    "                             savefig=os.path.join(figure_dir, '_'.join(['im' + initial_month_name, target_seas[l],region_of_interest, predictor_type, predictor_train_extent_name, 'CCA_forecast','CMORPH.png'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intdesk_train",
   "language": "python",
   "name": "intdesk_train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
